# Complete Project Tutorial: Unified Odds System

## üìö Table of Contents

1. [Project Overview](#project-overview)
2. [Architecture & Data Flow](#architecture--data-flow)
3. [Core Components Explained](#core-components-explained)
4. [Technical Techniques & Why We Use Them](#technical-techniques--why-we-use-them)
5. [Web Scraping Implementation](#web-scraping-implementation)
6. [API Design & Caching Strategy](#api-design--caching-strategy)
7. [Frontend Implementation](#frontend-implementation)
8. [Deployment & Automation](#deployment--automation)

---

## üéØ Project Overview

### What This Project Does

This is a **sports betting odds aggregator** that:

- Collects odds from multiple bookmakers (1xBet, FanDuel, Bet365)
- Scrapes OddPortal for multi-sport odds (Football, Basketball, Hockey, etc.)
- Provides a unified API to access all odds data
- Displays data in a modern, real-time web interface

### Why This Project Exists

**Problem**: Bettors need to check multiple websites to find the best odds.
**Solution**: Aggregate all odds in one place, compare them, and show the best value.

---

## üèóÔ∏è Architecture & Data Flow

### High-Level System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    UNIFIED ODDS SYSTEM                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Data Layer  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   API Layer  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   UI Layer   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚ñ≤                        ‚îÇ                         ‚îÇ
      ‚îÇ                        ‚îÇ                         ‚îÇ
      ‚îÇ                        ‚ñº                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Scrapers  ‚îÇ          ‚îÇ  Cache/JSON  ‚îÇ         ‚îÇ  WebSocket   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ   Storage    ‚îÇ         ‚îÇ   Updates    ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Complete Data Flow (Step by Step)

```
1. DATA COLLECTION
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  OddPortal      ‚îÇ‚îÄ‚îÄ‚îê
   ‚îÇ  (Playwright)   ‚îÇ  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
                        ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  OddsMagnet     ‚îÇ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Scrapers collect    ‚îÇ
   ‚îÇ  (API)          ‚îÇ  ‚îÇ    ‚îÇ  raw data            ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ              ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ              ‚ñº
   ‚îÇ  1xBet, etc     ‚îÇ‚îÄ‚îÄ‚îò    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  (Future)       ‚îÇ       ‚îÇ  Convert to unified  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ  JSON format         ‚îÇ
                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
2. DATA STORAGE                        ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  JSON Files (Fast, No Database Needed)       ‚îÇ
   ‚îÇ  - oddportal_unified.json                    ‚îÇ
   ‚îÇ  - unified_odds.json (OddsMagnet)            ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
3. API LAYER           ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  FastAPI (Python) - Serves Data via REST     ‚îÇ
   ‚îÇ  GET /oddsmagnet/api/oddportal               ‚îÇ
   ‚îÇ  GET /oddsmagnet/api/football/top10          ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
4. REAL-TIME UPDATES   ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  WebSocket Connection (for live updates)     ‚îÇ
   ‚îÇ  Pushes changes without page refresh         ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
5. USER INTERFACE      ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  HTML/JavaScript Frontend                    ‚îÇ
   ‚îÇ  - Displays odds in tables                   ‚îÇ
   ‚îÇ  - Highlights best odds                      ‚îÇ
   ‚îÇ  - Filters by sport, date, league            ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Core Components Explained

### 1. **OddPortal Scraper** (`bookmakers/oddportal/working_scraper.py`)

**What it does**: Scrapes live odds from OddPortal website for multiple sports.

**How it works**:

```python
# Step 1: Launch headless browser
browser = playwright.chromium.launch(headless=True)

# Step 2: Visit OddPortal website
page.goto('https://www.oddsportal.com/football/england/premier-league/')

# Step 3: Extract match links
match_links = page.query_selector_all('a[href*="/football/"]')

# Step 4: For each match, extract odds from multiple bookmakers
odds_data = extract_bookmaker_odds(match_page)

# Step 5: Save to JSON
save_to_json(matches_data)
```

**Key Features**:

- **Parallel Processing**: Scrapes 2 sports simultaneously (faster!)
- **Browser Stealth**: Mimics real user to avoid detection
- **Auto-save**: Saves data every 2 seconds (prevents data loss)
- **Zombie Process Prevention**: Kills orphaned Chrome processes

### 2. **OddPortal Collector** (`bookmakers/oddportal/oddportal_collector.py`)

**What it does**: Wrapper that runs the scraper continuously and converts data to unified format.

**Why we need it**:

```
Raw OddPortal Data ‚Üí Converter ‚Üí Unified Format ‚Üí API can serve it
(different structure)           (standard format)   (consistent access)
```

**How it works**:

```python
while True:  # Continuous loop
    # 1. Run scraper
    scraper.scrape_all()

    # 2. Convert to unified format
    unified_data = convert_to_unified_format(raw_data)

    # 3. Save unified JSON
    save_json(unified_data, 'oddportal_unified.json')

    # 4. Wait before next collection
    sleep(300)  # 5 minutes
```

### 3. **Unified API Server** (`core/live_odds_viewer_clean.py`)

**What it does**: Serves all odds data via REST API endpoints.

**Technology**: FastAPI (Python web framework)

**Why FastAPI**:

- ‚úÖ Extremely fast (async support)
- ‚úÖ Automatic API documentation
- ‚úÖ Type validation built-in
- ‚úÖ WebSocket support

**Key Endpoints**:

```python
# Get OddPortal data
@app.get("/oddsmagnet/api/oddportal")
async def get_oddportal_data():
    data = load_json('oddportal_unified.json')
    return JSONResponse(data)

# Get OddsMagnet football data
@app.get("/oddsmagnet/api/football/top10")
async def get_football_data():
    data = load_json('unified_odds.json')
    return JSONResponse(data)
```

### 4. **Frontend Viewer** (`html/oddsmagnet_viewer.html`)

**What it does**: Displays odds in a beautiful, interactive web interface.

**Features**:

- üìä Live odds comparison
- üîç Smart filtering (sport, league, date)
- üìÖ Calendar date picker
- üîÑ Real-time WebSocket updates
- üìå Bookmark favorite matches
- üé® Best odds highlighted in blue

---

## üõ†Ô∏è Technical Techniques & Why We Use Them

### 1. **Web Scraping with Playwright**

**What**: Playwright is a browser automation library.

**Why not just HTTP requests**?

```
Traditional HTTP:
‚ùå Can't handle JavaScript-rendered content
‚ùå Easy to detect as bot
‚ùå Can't interact with dynamic elements

Playwright:
‚úÖ Full browser simulation
‚úÖ Handles JavaScript (SPAs, dynamic loading)
‚úÖ Can click, scroll, fill forms
‚úÖ Looks like a real user
```

**How we use it**:

```python
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    # Launch browser in headless mode (no UI)
    browser = p.chromium.launch(headless=True)

    # Create context (like incognito window)
    context = browser.new_context(
        viewport={'width': 1920, 'height': 1080},
        user_agent='Mozilla/5.0...'  # Pretend to be real browser
    )

    # Create page and navigate
    page = context.new_page()
    page.goto('https://www.oddsportal.com/...')

    # Wait for content to load
    page.wait_for_selector('a[href*="/football/"]')

    # Extract data
    matches = page.query_selector_all('.match-link')
```

**Anti-Detection Techniques**:

```python
args=[
    '--disable-blink-features=AutomationControlled',  # Hide automation
    '--disable-dev-shm-usage',  # Prevent crashes
    '--no-sandbox',  # Linux compatibility
    '--user-agent=Mozilla/5.0...'  # Real browser UA
]
```

### 2. **Parallel Processing with ThreadPoolExecutor**

**What**: Run multiple scrapers at the same time.

**Why**:

```
Sequential (Slow):
Sport 1: 10 min ‚îÄ‚îÄ‚ñ∂ Sport 2: 10 min ‚îÄ‚îÄ‚ñ∂ Sport 3: 10 min = 30 min total

Parallel (Fast):
Sport 1: 10 min ‚îÄ‚îÄ‚îê
Sport 2: 10 min ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∂ All done = 10 min total
Sport 3: 10 min ‚îÄ‚îÄ‚îò
```

**How we implement it**:

```python
from concurrent.futures import ThreadPoolExecutor

# Process 2 sports at the same time
with ThreadPoolExecutor(max_workers=2) as executor:
    # Submit tasks
    future1 = executor.submit(scrape_sport, 'football')
    future2 = executor.submit(scrape_sport, 'basketball')

    # Get results as they complete
    for future in as_completed([future1, future2]):
        result = future.result()
```

**Why only 2 parallel workers**?

- üß† Each browser uses ~200MB RAM
- üñ•Ô∏è Too many = system overload
- ‚öñÔ∏è 2 = Perfect balance of speed and stability

### 3. **JSON-Based Storage (No Database)**

**Why JSON instead of database**?

```
Database (PostgreSQL/MySQL):
‚ùå Need to install/maintain database server
‚ùå Complex queries for simple data
‚ùå Overhead for schema migrations
‚ùå Overkill for read-heavy use case

JSON Files:
‚úÖ No setup required
‚úÖ Human-readable
‚úÖ Fast for read operations
‚úÖ Easy backups (just copy file)
‚úÖ Works perfectly for our use case
```

**How we structure JSON**:

```json
{
  "sport": "multi",
  "timestamp": "2026-01-13T23:59:39.381180",
  "matches_count": 128,
  "matches": [
    {
      "name": "team a v team b",
      "datetime": "2026-01-13 18:00:00",
      "sport": "football",
      "league": "Premier League",
      "markets": {
        "popular markets": [
          {
            "name": "win market",
            "odds": [
              {
                "bookmaker_name": "1xBet",
                "decimal_odds": 3.7,
                "selection": "Team A"
              }
            ]
          }
        ]
      }
    }
  ]
}
```

### 4. **ETag Caching Strategy**

**What**: ETag is a cache validation mechanism.

**How it works**:

```
1. Client requests: GET /api/oddportal
2. Server responds with:
   - Data
   - ETag: "abc123" (hash of file timestamp + size)

3. Client caches the response

4. Next request, client sends: If-None-Match: "abc123"
5. Server checks: Has file changed?
   - No ‚Üí Return 304 Not Modified (no data transfer!)
   - Yes ‚Üí Return new data with new ETag
```

**Our implementation**:

```python
# Calculate ETag based on file modification
file_stat = oddportal_file.stat()
etag = hashlib.md5(f"{file_stat.st_mtime}-{file_stat.st_size}".encode()).hexdigest()

# Check if client has latest version
if request.headers.get('if-none-match') == etag:
    return Response(status_code=304)  # Not modified

# Return fresh data
return JSONResponse(data, headers={'ETag': etag})
```

**Why we DISABLED it**:

```python
# We disabled ETag to always serve fresh data
# Because: Odds change frequently, users need latest data always
headers={
    'Cache-Control': 'no-store, no-cache, must-revalidate',
    'Pragma': 'no-cache',
    'Expires': '0'
}
```

### 5. **WebSocket for Real-Time Updates**

**What**: Bidirectional communication channel (server can push to client).

**Traditional HTTP vs WebSocket**:

```
HTTP Polling (Old Way):
Client ‚îÄ‚îÄrequest‚îÄ‚îÄ‚ñ∂ Server
       ‚óÄ‚îÄ‚îÄresponse‚îÄ‚îò
[wait 5 seconds]
Client ‚îÄ‚îÄrequest‚îÄ‚îÄ‚ñ∂ Server
       ‚óÄ‚îÄ‚îÄresponse‚îÄ‚îò
‚ùå Inefficient, delay in updates

WebSocket (Our Way):
Client ‚ïê‚ïê‚ïêconnection‚ïê‚ïê‚ïê Server
       ‚óÄ‚îÄ‚îÄpush update‚îÄ‚îÄ‚îò (instant!)
       ‚óÄ‚îÄ‚îÄpush update‚îÄ‚îÄ‚îò (instant!)
‚úÖ Real-time, efficient
```

**Implementation**:

```javascript
// Frontend (JavaScript)
const ws = new WebSocket("ws://localhost:8000/ws");

ws.onmessage = (event) => {
  const data = JSON.parse(event.data);
  updateUI(data); // Instant update!
};
```

```python
# Backend (Python)
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()

    while True:
        # When data changes, push to client
        if data_changed:
            await websocket.send_json(new_data)
        await asyncio.sleep(1)
```

### 6. **Progressive Rendering for Performance**

**Problem**: Rendering 1000+ matches freezes the browser.

**Solution**: Render in chunks (batches).

```javascript
// Bad: Render all at once
function renderAll(matches) {
  matches.forEach((match) => {
    table.appendChild(createRow(match)); // Blocks UI!
  });
}

// Good: Render in chunks
async function renderProgressive(matches) {
  const CHUNK_SIZE = 50;

  for (let i = 0; i < matches.length; i += CHUNK_SIZE) {
    const chunk = matches.slice(i, i + CHUNK_SIZE);

    // Render chunk
    chunk.forEach((match) => table.appendChild(createRow(match)));

    // Yield to browser (keeps UI responsive)
    await new Promise((resolve) => setTimeout(resolve, 0));
  }
}
```

**Why this works**:

- Breaks large task into small pieces
- Browser can handle other events between chunks
- User sees data appearing progressively (better UX)

### 7. **Debouncing for Search/Filters**

**Problem**: User typing "Liverpool" triggers search on every keystroke:

```
L ‚Üí search (250 results)
Li ‚Üí search (180 results)
Liv ‚Üí search (50 results)
Live ‚Üí search (12 results)
Liver ‚Üí search (3 results)
Liverp ‚Üí search (2 results)
Liverpo ‚Üí search (1 result)
Liverpool ‚Üí search (1 result)
‚ùå 8 searches for one input!
```

**Solution**: Debounce (wait until user stops typing).

```javascript
function debounce(func, wait) {
  let timeout;
  return function (...args) {
    clearTimeout(timeout);
    timeout = setTimeout(() => func(...args), wait);
  };
}

// Only search 300ms after user stops typing
const debouncedSearch = debounce(performSearch, 300);
searchInput.addEventListener("input", debouncedSearch);
```

**Result**: Only 1 search when user finishes typing!

---

## üï∑Ô∏è Web Scraping Implementation

### OddPortal Scraping Strategy

**Challenge**: OddPortal has many sports and leagues. Can't scrape everything.

**Our Approach**:

1. **Focus on popular leagues only**
2. **Limit matches per league** (top 50)
3. **Scrape 2 sports in parallel**

**Configuration**:

```python
self.leagues = {
    'football': [
        'https://www.oddsportal.com/football/england/premier-league/',
        'https://www.oddsportal.com/football/spain/laliga/',
        'https://www.oddsportal.com/football/germany/bundesliga/',
        # ... more top leagues
    ],
    'basketball': [
        'https://www.oddsportal.com/basketball/usa/nba/',
        'https://www.oddsportal.com/basketball/europe/euroleague/',
        # ... more leagues
    ]
}
```

### Match Extraction Process

**Step-by-Step**:

```python
def scrape_league(league_url):
    # 1. Go to league page
    page.goto(league_url)

    # 2. Find all match links
    match_links = []
    all_links = page.query_selector_all('a[href]')

    for link in all_links:
        href = link.get_attribute('href')

        # 3. Validate if it's a match link
        if is_valid_match_url(href):
            match_links.append(href)

    # 4. Scrape each match in parallel
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = []
        for url in match_links[:50]:  # Limit to 50
            future = executor.submit(scrape_match, url)
            futures.append(future)

        # 5. Collect results
        for future in as_completed(futures):
            match_data = future.result()
            save_match(match_data)
```

### Odds Extraction

**HTML Structure** (simplified):

```html
<div class="odds-table">
  <div class="bookmaker">
    <span class="name">1xBet</span>
    <span class="odd home">3.70</span>
    <span class="odd draw">3.40</span>
    <span class="odd away">2.10</span>
  </div>
</div>
```

**Extraction Code**:

```python
def extract_odds(page):
    bookmakers = []

    # Find all bookmaker rows
    rows = page.query_selector_all('.odds-table .bookmaker')

    for row in rows:
        # Extract bookmaker name
        name = row.query_selector('.name').text_content()

        # Extract odds
        home = row.query_selector('.odd.home').text_content()
        draw = row.query_selector('.odd.draw').text_content()
        away = row.query_selector('.odd.away').text_content()

        bookmakers.append({
            'name': name,
            'home_odds': float(home),
            'draw_odds': float(draw),
            'away_odds': float(away)
        })

    return bookmakers
```

### Zombie Process Prevention

**Problem**: Each browser instance starts Chrome processes. If not closed properly ‚Üí zombies!

**Solution**:

```python
import atexit
import signal

class Scraper:
    def __init__(self):
        self.active_browsers = []
        self.active_contexts = []

        # Register cleanup on exit
        atexit.register(self.cleanup_all_browsers)
        signal.signal(signal.SIGTERM, self._signal_handler)

    def cleanup_all_browsers(self):
        """Kill all Chrome processes"""
        # Close tracked browsers
        for browser in self.active_browsers:
            browser.close()

        # Kill zombie Chrome processes
        import psutil
        for proc in psutil.process_iter(['name', 'cmdline']):
            if 'chrome' in proc.info['name'].lower():
                if 'headless' in ' '.join(proc.info['cmdline']):
                    proc.kill()  # Kill it!
```

**Why this matters**:

- Without cleanup: 100s of Chrome processes accumulate
- With cleanup: Always clean system

---

## üîå API Design & Caching Strategy

### RESTful API Design

**Endpoint Structure**:

```
GET /oddsmagnet/api/oddportal          # All OddPortal data
GET /oddsmagnet/api/oddportal?sport=basketball  # Filter by sport
GET /oddsmagnet/api/football/top10     # OddsMagnet football
GET /docs                              # Auto-generated docs
```

**Query Parameters**:

```python
@app.get("/oddsmagnet/api/oddportal")
async def get_oddportal_data(
    page: int = 1,           # Pagination
    page_size: int = 999,    # Items per page
    sport: str = None,       # Filter: 'basketball', 'football'
    league: str = None,      # Filter: 'premier-league'
    search: str = None       # Search in match names
):
    # Load data
    data = load_json('oddportal_unified.json')
    matches = data['matches']

    # Apply filters
    if sport:
        matches = [m for m in matches if m['sport'] == sport]

    if league:
        matches = [m for m in matches if league in m['league'].lower()]

    if search:
        matches = [m for m in matches if search.lower() in m['name'].lower()]

    # Paginate
    start = (page - 1) * page_size
    end = start + page_size

    return {
        'matches': matches[start:end],
        'total': len(matches),
        'page': page
    }
```

### Response Format

**Standardized Structure**:

```json
{
  "sport": "multi",
  "timestamp": "2026-01-13T23:59:39",
  "source": "oddportal",
  "matches_count": 128,
  "total_matches": 128,
  "current_page": 1,
  "matches": [...]
}
```

**Why this structure**:

- ‚úÖ `timestamp`: Client knows data freshness
- ‚úÖ `matches_count`: Shows filtered count
- ‚úÖ `total_matches`: Shows total available
- ‚úÖ `source`: Know where data came from

### CORS Configuration

**What is CORS**: Cross-Origin Resource Sharing (security feature).

**Why we need it**:

```
Without CORS:
Frontend (localhost:8000) ‚îÄ‚îÄ‚úó‚îÄ‚îÄ‚ñ∂ API (localhost:8000)  ‚ùå Blocked!

With CORS:
Frontend (localhost:8000) ‚îÄ‚îÄ‚úì‚îÄ‚îÄ‚ñ∂ API (localhost:8000)  ‚úÖ Allowed!
```

**Configuration**:

```python
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins (or specify domains)
    allow_credentials=True,
    allow_methods=["*"],  # Allow all HTTP methods
    allow_headers=["*"],  # Allow all headers
)
```

---

## üé® Frontend Implementation

### Data Fetching Strategy

**Early Fetch Optimization**:

```javascript
// Start fetching BEFORE page finishes loading
window.earlyDataFetch = fetch("/oddsmagnet/api/football/top10", {
  cache: "no-store",
}).then((r) => r.json());

// When page ready, use already-fetched data
window.addEventListener("DOMContentLoaded", async () => {
  const data = await window.earlyDataFetch;
  renderMatches(data.matches); // Instant display!
});
```

**Why**: Page loads faster (data fetching overlaps with page rendering).

### Date Picker Implementation

**Challenge**: Show which dates have matches.

**Solution**:

```javascript
function renderCalendar() {
  // 1. Collect all unique dates from matches
  const datesWithMatches = new Set();
  allMatches.forEach((match) => {
    const date = new Date(match.datetime);
    date.setHours(0, 0, 0, 0);
    datesWithMatches.add(date.toDateString());
  });

  // 2. Render calendar
  for (let day = 1; day <= 31; day++) {
    const dayElement = document.createElement("div");
    dayElement.textContent = day;

    // 3. Highlight if has matches
    const dayDate = new Date(currentYear, currentMonth, day);
    if (datesWithMatches.has(dayDate.toDateString())) {
      dayElement.classList.add("has-matches");
      dayElement.style.background = "rgba(96, 165, 250, 0.4)";
    }

    calendar.appendChild(dayElement);
  }
}
```

### Best Odds Highlighting

**Algorithm**:

```javascript
function findBestOdds(matches) {
  matches.forEach((match) => {
    // Group odds by selection (Home, Draw, Away)
    const oddsBySelection = {};

    match.markets.forEach((market) => {
      market.odds.forEach((odd) => {
        if (!oddsBySelection[odd.selection]) {
          oddsBySelection[odd.selection] = [];
        }
        oddsBySelection[odd.selection].push(odd);
      });
    });

    // Find highest odd for each selection
    Object.keys(oddsBySelection).forEach((selection) => {
      const odds = oddsBySelection[selection];
      const maxOdd = Math.max(...odds.map((o) => o.decimal_odds));

      // Mark best odds
      odds.forEach((odd) => {
        if (odd.decimal_odds === maxOdd) {
          odd.is_best = true;
        }
      });
    });
  });
}
```

**Visual Highlighting**:

```css
.best-odds {
  background: linear-gradient(135deg, #3b82f6, #2563eb);
  color: white;
  font-weight: bold;
  box-shadow: 0 0 10px rgba(59, 130, 246, 0.5);
}
```

### Filtering Implementation

**Multi-Filter Logic**:

```javascript
function applyFilters() {
  let filtered = allMatches;

  // 1. Sport filter (OddPortal only)
  if (currentSport === "oddportal" && selectedOddportalSport !== "all") {
    filtered = filtered.filter((m) => m.sport === selectedOddportalSport);
  }

  // 2. League filter
  if (selectedLeague !== "all") {
    filtered = filtered.filter((m) => m.league === selectedLeague);
  }

  // 3. Date filter
  if (selectedDate !== "all") {
    filtered = filtered.filter((m) => {
      const matchDate = new Date(m.datetime);
      matchDate.setHours(0, 0, 0, 0);
      return matchDate.toDateString() === selectedDate;
    });
  }

  // 4. Search filter
  if (searchQuery) {
    filtered = filtered.filter(
      (m) =>
        m.name.toLowerCase().includes(searchQuery.toLowerCase()) ||
        m.league.toLowerCase().includes(searchQuery.toLowerCase())
    );
  }

  renderMatches(filtered);
}
```

---

## üöÄ Deployment & Automation

### GitHub Actions CI/CD

**What**: Automatically deploy code when you push to GitHub.

**Workflow** (`.github/workflows/deploy.yml`):

```yaml
name: Deploy to VPS

on:
  push:
    branches: [main] # Trigger on push to main branch

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      # 1. SSH into VPS
      - name: Deploy to VPS
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.VPS_HOST }}
          username: ${{ secrets.VPS_USER }}
          key: ${{ secrets.SSH_KEY }}

          # 2. Pull latest code
          script: |
            cd /home/ubuntu/services/unified-odds
            git pull origin main

            # 3. Restart services
            systemctl restart unified-odds
```

**Process**:

```
Local: git push origin main
   ‚Üì
GitHub: Receives push
   ‚Üì
GitHub Actions: Starts workflow
   ‚Üì
VPS: Pulls code, restarts services
   ‚Üì
Live: Updated site!
```

**Why this is awesome**:

- ‚úÖ No manual deployment
- ‚úÖ Every push = automatic update
- ‚úÖ Fast deployments (30 seconds)

### Systemd Service Configuration

**What**: Linux service manager (keeps apps running).

**Service File** (`unified-odds.service`):

```ini
[Unit]
Description=Unified Odds System
After=network.target

[Service]
Type=simple
User=ubuntu
WorkingDirectory=/home/ubuntu/services/unified-odds
ExecStart=/home/ubuntu/services/unified-odds/venv/bin/python core/live_odds_viewer_clean.py
Restart=always  # Auto-restart if crashes
RestartSec=10   # Wait 10 seconds before restart

[Install]
WantedBy=multi-user.target
```

**Commands**:

```bash
# Start service
systemctl start unified-odds

# Enable auto-start on boot
systemctl enable unified-odds

# Check status
systemctl status unified-odds

# View logs
journalctl -u unified-odds -f
```

### Continuous Data Collection

**OddPortal Collector** runs as separate service:

```bash
# Start collector (runs forever)
python bookmakers/oddportal/oddportal_collector.py --continuous --interval 300

# Runs every 5 minutes:
# 1. Scrape all sports
# 2. Save to JSON
# 3. Wait 5 minutes
# 4. Repeat
```

**Why continuous**:

- Odds change frequently (need fresh data)
- Users see latest odds without manual updates

---

## üéì Key Learnings & Best Practices

### 1. **Choose Right Tool for the Job**

```
Simple websites ‚Üí HTTP requests (fast)
JavaScript-heavy ‚Üí Playwright (necessary)
Real-time data ‚Üí WebSocket (efficient)
Static data ‚Üí REST API (simple)
```

### 2. **Performance Optimization**

```python
# ‚ùå Bad: Load everything at once
matches = load_all_matches()  # 10,000 matches
render_all(matches)  # Browser freezes!

# ‚úÖ Good: Pagination + Progressive rendering
matches = load_paginated(page=1, size=100)
render_progressive(matches)  # Smooth!
```

### 3. **Error Handling**

```python
# ‚ùå Bad: No error handling
def scrape_match(url):
    page.goto(url)
    return extract_data(page)

# ‚úÖ Good: Try-catch with fallback
def scrape_match(url):
    try:
        page.goto(url, timeout=30000)
        return extract_data(page)
    except TimeoutError:
        print(f"Timeout: {url}")
        return None
    except Exception as e:
        print(f"Error: {e}")
        return None
```

### 4. **Resource Cleanup**

```python
# ‚ùå Bad: No cleanup
browser = launch_browser()
scrape_data()
# Browser process still running!

# ‚úÖ Good: Always cleanup
try:
    browser = launch_browser()
    scrape_data()
finally:
    browser.close()  # Always closes!
```

### 5. **Logging for Debugging**

```python
# ‚úÖ Good logging
print(f"[{timestamp}] Starting scrape: {sport}")
print(f"  ‚úì Found {len(matches)} matches")
print(f"  ‚ö†Ô∏è  Warning: {error}")
print(f"  ‚ùå Error: {critical_error}")
```

---

## üìä Project Statistics

### Performance Metrics

```
Data Collection:
- OddPortal: ~180 matches in 8-10 minutes
- Parallel sports: 2x faster than sequential
- Auto-save frequency: Every 2 seconds

API Performance:
- Response time: <50ms (JSON file read)
- WebSocket latency: <10ms
- Concurrent users: Handles 100+ easily

Frontend Performance:
- Initial load: <500ms (with early fetch)
- Progressive render: 50 matches/chunk
- Smooth scrolling: 60 FPS
```

### Code Organization

```
Lines of Code:
- Scrapers: ~800 lines
- API: ~3,600 lines
- Frontend: ~4,900 lines
- Total: ~9,300 lines

Files:
- Python: 15 files
- JavaScript: 1 large file (viewer)
- JSON configs: 3 files
- Documentation: 10 markdown files
```

---

## üéØ Conclusion

This project demonstrates:

1. **Web Scraping** at scale with anti-detection
2. **Parallel Processing** for speed
3. **RESTful API** design
4. **Real-time Updates** via WebSocket
5. **Modern Frontend** with progressive rendering
6. **DevOps** with CI/CD automation

**Key Takeaway**: Combine the right technologies to solve real problems efficiently!

---

## üìö Further Reading

- [Playwright Documentation](https://playwright.dev)
- [FastAPI Documentation](https://fastapi.tiangolo.com)
- [WebSocket Protocol](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API)
- [RESTful API Design](https://restfulapi.net)
- [GitHub Actions](https://docs.github.com/en/actions)

---

**Last Updated**: January 14, 2026
**Project Version**: 2.0
**Author**: Unified Odds System Team
